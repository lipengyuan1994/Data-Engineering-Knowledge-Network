2. Compliance with regulations requires that you keep copies of logs generated by applications that perform financial transactions for 3 years.  You currently run applications on-premises but will move them to Google Cloud. You want to keep the logs for three years as inexpensively as possible. You do not expect to query the logs but must be able to provide access to files on demand. How would you configure GCP resources to meet this requirement?

The correct answer is to send application logs to [[Cloud Logging]] and create a [[Cloud Storage]] sink to store the logs for the long term. Logging stores logs for 30 days so leaving the logs in Cloud Logging will not meet the requirements.  Cloud Logging does not support data lifecycle management policies, Cloud Storage does. Bigtable is not a sink option for Cloud Logging.


3. A team of developers is consolidating several data pipelines used by an insurance company to process claims. The claims processing logic is **complex** and already encoded in a **Java** library. The current data pipelines run in **batch** mode but the insurance company wants to process claims as soon as they are created. What GCP service would you recommend using?

The correct answer is [[Cloud Dataflow]], which supports **both batch and stream processing** and can execute Java as part of the dataflow.  Cloud Datastore is a document database and not suitable for dataflow processing. [[Cloud Dataprep]] is used to prepare data for analysis and machine learning.  [[Cloud PubSub]] is for messaging and does not support complex processing logic.


4. A regional auto dealership is migrating its business applications to Google Cloud. The company currently uses a third-party application that uses PostgreSQL for storing data.  The CTO wants to reduce the cost of supporting this application and the database. What would you recommend to the CTO as the best option to reduce the cost of maintaining and operating the database?

he best option is to use [[Cloud SQL]], which is a managed database service that supports PostgreSQL.[[ Cloud Datastore]] is not an option because it is a managed document database, not a relational database service. [[Cloud Spanner]] is not required because there are no global or multi-regional requirements and so Cloud SQL is a more cost effective solution. Switching to SQL Server without using a managed database service will not eliminate the need to maintain and operate a database.



5. You are developing a machine learning model to predict the likelihood of a device failure. The device generates a stream of metrics every thirty seconds. The metrics include 3 categorical values, 5 integer values, and 1 floating point value. The floating point value ranges from 0 to 100.  For the purposes of the model, the floating point value is more precise than needed.  Mapping that value to a feature with possible values "high", "medium", and "low" is sufficient. What feature engineering technique would you use to transform the floating point value to high, medium, or low?

The correct answer is **bucketing**. In this case, values from 0 to 33 could be low, 34 to 66 could be medium, and values greater than 66 could be high. Regularization is the limiting of information captured by a model to prevent overfishing; L1 and L2 are two examples of regularization techniques. Clustering is an unsupervised learning technique for identifying groups of similar entities. Normalization is a transformation that scales numeric values to the range 0 1o 1.



6. A company is migrating it's backend services to Google Cloud. Services are implemented in Java and Kafka is used as a messaging platform between services. The DevOps team would like to reduce their operational overhead. What managed GCP service might they use as an alternative to Kafka?

The correct answer is [[Cloud PubSub]], which is a managed messaging service. [[Cloud Dataflow]] is a stream and batch processing platform, not a messaging service. [[Cloud Dataproc]] is a managed Spark/Hadoop service. Cloud Datastore is a document database.


7. You are implementing a data warehouse using BigQuery. A data modeler, unfamiliar with BigQuery, developed a model that is highly normalized. You are concerned that a highly normalized model will require the frequent joining of tables to respond to common queries. **You want to denormalize** the data model but still want to be able to represent 1-to-many relations. How could you do this with BigQuery?

The correct answer is to store the associated data in **ARRAYs**, since the associated data are all of the same time. [[Partitioning]] and [[clustering]] are physical [[data modeling]] techniques for **improving query performance** and not related to denormalizing the logical data model.



8. A team of machine learning engineers is developing deep learning models using Tensorflow.  They have extremely large data sets and must frequently retrain models.  They are currently using a managed instance group with a fixed number of VMs and they are not meeting SLAs for retraining. What would you suggest the machine learning engineers try next?

The correct answer is to attach GPUs to [[Compute Engine]] VMs to accelerate Tensorflow model training. Training [[Tensorflow]] models on very large data sets using CPUs is not as cost-efficient as using GPUs so scaling the VMs, either horizontally or vertically, is not as cost effective as using GPUs. Simply deploying the training workload to [[Kubernetes]] without accelerators will not significantly improve performance the way GPUs can.


9. You have a BigQuery table partitioned by ingestion time and want to create a view that returns only rows ingested in the last 7 days. Which of the following statements would you use in the WHERE clause of the view definition to limit results to include only the most recent seven days of data?

The correct answer is the statement that references `_PARTITIONTIME`, which is the pseudo-column created by BigQuery for tables partitioned on ingestion. `PARTIONTIME`, `_INGESTIONTIME`, and `INGESTIONTIME` are not the correct name of the pseudo column.


10. Your department currently uses [[HBase]] on a [[Hadoop]] cluster for an analytics database. You want to migrate that data to Google Cloud. There is only one workload run on the Hadoop cluster and uses the HBase API. You would like to avoid having to manage a [[Spark]] and Hadoop cluster but you do not want to change the application code of the one workload running on the cluster. How could you move the workload to GCP, use a managed service, and not change the application?

The correct answer is to migrate the data to [[Bigtable]] and use Bigtable's HBase API. [Cloud Storage], [Cloud Datastore], and [Bigquery] do not have an HBase API.



11. A data pipeline is not performing well enough to meet SLAs. You have determined that long running database queries are slowing processing. You decide to try to use a read-through cache. You want the cache to support sets and sorted sets as well.  What Google Cloud Service would you use?

The correct answer is [[Cloud Memorystore]] with [[Redis]]. [[Memcache]] does not support sets or sorted sets.  Cloud Memorystore does not support SQL Server, which is a relational database not a cache.  Cloud Datastore is not a cache, it is a managed document database.


12. Sensor data from manufacturing machines is ingested thru Pub/Sub and read by a Cloud Dataflow job, which analyzes the data. The data arrives in one-minute intervals and includes a timestamp and measures of temperature, vibration, and ambient humidity.  Industrial engineers have determined if the average temperature exceeds 10% of the maximum safe operating temperature for more than 10 minutes and average ambient humidity is above 90% for more than 10 minutes then the machine should be shut down.  What operation would you perform on the stream of data to determine when to trigger an alert to shutdown the machine?

The correct answer is to create a sliding window and trigger an alert based on averages of the sliding window temperature and humidity values.  Tumbling windows should not be used because they do not yield all possible windows between two time points.  A watermark is used to denote a point in a stream when no data in the rest of the stream will be older than the time specified in the watermark.  There is no need to create a custom windowing mechanism using Redis and a Java or Python function since [[Cloud Dataflow]] implements sliding window functions.

13. Autonomous vehicles stream data about vehicle performance to a Cloud Pub/Sub queue for ingestion. You want to randomly sample the data stream to collect 0.01% of the data for your own analysis.  You want to do this with the least amount of new code and infrastructure while still having access to the data as soon as possible. What is the best option for doing this?

The correct answer is to create a [[Cloud Function]] that executes when a message is written to the Cloud Pub/Sub topic, randomly generate a number between 0 and 1 in the function and if the random number is less than 0.01, then write the message to another topic that you created to act as the source of data for your analysis. Cloud Pub/Sub does not have a specialized sink mechanism for writing to Cloud Storage and writing to a file and processing the data on an hourly basis does not meet the requirement of processing the data as soon as possible.  Using an App Engine application to poll the topic continuously is less efficient than using Cloud Function to process the data when a message is written to the topic.  There is no requirement to have a globally scalable relational database for this processing and Cloud Spanner is an expensive service so it should not be part of a solution.


14. You are training a deep learning neural network. You are using gradient descent to find optimal weights.  You want to update the weights after each instance is analyzed. Which type of gradient descent would you use?

The correct answer is **stochastic [[gradient descent**]] because **it updates after each instance is analyzed**. Batch gradient descent is incorrect because it updates after processing the entire training set. Mini-batch is incorrect because it **updates after multiple instance**s. (The number of instances is less than the number of instances in the training set.)  Max-batch gradient descent is not the name of a form of gradient descent.


15. You are designing a [[Bigtable]] database for a multi-tenant analytics service that requires low latency writes at extremely high volumes of data. As an experienced relational data modeler, you are familiar with the process of normalizing data models. Your data model consists of 15 tables with each table have at most 20 columns.  Your initial tests with 20% of expected load indicate much higher than expected latency and some potential issues with connection overhead. What can you do to address these problems?

The correct answer is denormalize the data model to use a single, wide-column table in Bigtable to reduce latency and address connection issues.  Using multiple small tables in Bigtable is not advised because it increases latency and can lead to connection overhead issues.  Further normalizing the data model may increase the problems.  [[BigQuery]] is not a good option because, although it is an analytical database, the requirement of low latency, high volume writes makes Bigtable a better option for this use case.




16. You are designing a **time series database**. Data will arrive from thousands of sensors at one-minute intervals. You want to model this time series data using recommended practices. Which of the following would you implement?

The correct answer is design rows to store the set of measurements from one sensor at one point in time. These are known as **[[narrow tables]]**. Designing rows to store measurements from one sensor over an hour is a wide-table pattern but not recommended for time series data. Designing rows to store measurements from all sensors at a single time point would be a wide table pattern and **require waiting** for all or most of the sensor values to arrive before writing the data. Designing rows up to the maximum size of 100 MB per row would also lead to a wide table pattern.

17. You are migrating a [[data warehouse]] to [[BigQuery]] and want to optimize the data types using in BigQuery. You have many columns in the existing data warehouse that store absolute point in time values.  They are implemented using 8-byte integers in the existing data warehouse. What data type would you use in BigQuery?

The correct answer is to use a **timestamp** to represent an absolute point in time. Long integer is not an available data type, although INT64 and NUMERIC could be used to represent long integers, i.e. 8-byte integers.  Datetime is used for data and time independent of timezone.  Time is used to represent clock time.

18. Your organization is migrating an enterprise [[data warehouse]] from an on-premises [[PostgreSQL]] database to Google Cloud to use [[BigQuery]].  The data warehouse is used by 7 different departments each of which has its own data, workloads, and reports.  You would like to follow recommended data warehouse migration practices. Which of the following procedures would you follow as the first steps in the migration process?

The correct answer is to transfer groups of tables related to one use case at a time. Do not modify tables in the process. Configure and test downstream processes to read from BigQuery.  Exporting all data at once is not recommended. Transferring all reporting jobs and workloads at once is not recommended.  Modifying tables at this point in the migration, including denormalizing, is not recommended. Bigtable is not an analytical database and should not be used as the target database of a data warehouse housed in a relational database, in this case PostgreSQL.



19. The Chief Finance Officer of your company has requested a set of [[data warehouse]] reports for use by end users who are not proficient in SQL. You want to use Google Cloud Services. Which of the following are services you could use to create the reports?

The correct answers are [[Looker]] and [[Data Studio]], which are both reporting and visualization services. Tableau is incorrect because it is not a Google Cloud Platform service.  [[Cloud Dataprep]] and [[Cloud Fusion]] are used to prepare and process data prior to analysis and are not reporting services.


20. A team of data scientists is using a [[Redis]] cache provided by [[Cloud Memorystore]] to store a large data set in memory.  they have a custom Python application for analyzing the data. While optimizing the program they found that a significant amount of time is spent counting the number of distinct elements in sets. They are will to use less precise numbers if they can get an approximate answer faster. Which Redis data type would you recommend they use?

The correct answer is  [[HyperLogLog]], which is a data structure that provides approximate distinct item counts to set with low latency. [[Sorted Sets]] will not reduce the time to found distinct items. There is no Stochastic Set data type in Redis. List will not provide approximate counts of distinct items.



21. You are migrating an on-premises Spark and Hadoop cluster to Google Cloud using [[Cloud Dataproc]]. The on-premises cluster uses [[HDFS]] and attached storage for persistence. The cluster runs continually, 24x7. You understand that it is common to use ephemeral Spark and Hadoop clusters in Google Cloud but are concerned about the time it would take to load data into HDFS each time a cluster is created. What would you do to ensure data is accessible to a new cluster as soon as possible?

The correct answer is to use the [[Cloud Storage Connector]]. This allows data to be directly accessed from Cloud Storage. Copying data from Cloud Storage or Bigtable is incorrect because it would take longer than using Cloud Storage Connector. [[Cloud Dataproc]] does not support specifying snapshots when creating a cluster.


22. A Spark job is failing but you cannot identify the problem from the contents of the log file. You want to run the job again and get more logging information. Which of the following command fragments would you use as part of a command to submit a job to Spark and have it **log more detail than the default amount**?

The correct answer is 'gcloud dataproc jobs submit spark --driver-log-levels'. 'gcloud dataproc submit jobs spark --driver-log-levels=debug' is incorrect because the order of 'jobs' and 'submit' are reversed. The other options are incorrect, there is no --enable-debug option.


23. You have migrated a Spark cluster from on-premises to [[Cloud Dataproc]]. You are following the best practice of using ephemeral clusters to keep costs down.  When the cluster starts, data is copied to the cluster HDFS before jobs start running. You would like to minimize the time between creating a cluster and starting jobs running on that cluster. Which of the following could do the most to reduce that time without increasing cost?
The correct answer is to use the [[Cloud Storage Connector]] and keep data in Cloud Storage instead of copying it each time to HDFS. Using SSDs could reduce latency if HDDs were used but data would still have to be copied and switching from HDD to SSD would increase costs. Using Cloud SQL would incur additional costs for the database and jobs would have to be modified to read data from a relational database instead of HDFS. Creating a managed instance group with persistent storage would increase the cost and overall complexity of the system.


24. A Python [[ETL]] process is loading a data warehouse is not meeting ingestion SLAs. The service that performs the ingestion and initial processing cannot keep up with incoming data at peak times. The peak times do not last longer than one minute and occur at most once per day but data is sometimes lost during those times. You need to ensure data is not lost to the ingestion process. What would you try first to prevent data loss?

The correct answer is to ingest data into a [[Cloud PubSub]] topic using a **pull** processing mode. Cloud Pub/Sub will scale as needed and will not lose data. By having the consuming service pull messages when it is able to, data is cached until it can be processed.  Rewriting an ETL process in another language could be difficult and time consuming and should not be tried before less drastic changes are evaluated. A push subscription is not appropriate since the consuming service presumably would not be able to process peak loads fast enough to keep up. Cloud Dataflow is used for stream and batch process and not scalable ingestion pipelines.



25. You support an ETL process on-premises and need to migrate it to a virtual machine running in Google Cloud. The process sometimes fails without warning. You do not have time to diagnose and correct the problem before migrating. What can you do to discover failure as soon as possible?

The correct answer is to create a [[Cloud Monitor ]]uptime check and if the uptime check fails send a notification to you. Creating an application on App Engine could work but it requires an additional service to maintain and would incur additional costs.  Creating an alert on CPU utilization falling below 5% is incorrect because CPU utilization dropping below 5% does not mean the process failed. There is no way to create an alert on Cloud Logging not receiving data from a service.



26. Many applications and services are running in several Google Cloud services. You would like to know if all services' logs are up to date with ingesting data into [[Cloud Logging]]. How would you get this information with the least effort?

The correct answer is view the Cloud Logging Resource page in Google Cloud Console. Writing a Python script or writing a custom Logs Viewer query would require additional work. The Cloud Logging Router page does not display this information.


28. A [[Cloud Dataflow]] job will need to list files and copy those files from a [[Cloud Storage]] bucket. What is the best way to ensure the job will have access when it tries to read data from those buckets?  The job will not write data to Cloud Storage.

The correct answer is to create a **service account** and grant it the Storage Object Viewer role, which is a predefined role with sufficient permissions to allow an entity to list files and read data from a bucket. **You cannot assign a role to a [[Cloud Dataflow]] job**. An application should use a service account type of entity, not a Cloud Identity. There is no need to create a custom role because Storage Object View is a predefined role that meets requirements. Also, the custom role should have storage.objects.list to get a list of contents of a bucket.



30. As an administrator of a [[BigQuery]] data warehouse, you grant access to users according to their responsibilities in the organization. You follow the [[Principle of Least Privilege]] when granting access. Several users need to be able to read and update data in a BigQuery table as well as delete tables in a dataset.   What role would you assign to those users?
roles/bigquery.dataEditor.

31. A colleague has asked for your advice about tuning a classifier built using random forests. What hyperparameter or hyperparameters would you suggest adjusting to improve accuracy?
The correct answer is the number of trees and the depth of trees are both hyperparameters that could be adjusted to improve accuracy. Random forest does not use a learning rate hyperparameter. Random forest do not use the concept of clusters.


32. When training a neural network, what parameter is learned?
The correct answer is the weight on input values to a node.

33. You are building a classifier to identify customers most likely to buy additional products when presented with an offer.  You have approximately 20 features.  The model is not performing as well as needed. You suspect the model is missing some relationships that are determined by a combination of two features. What feature engineering technique would you try to improve the quality of the model?
The correct answer is the feature cross, which is the Cartesian product (all possible combinations) of two variables. Normalization is a technique to map numeric values into a range of 0 to 1. Regularization is a technique to reduce overfitting.  Bucketing is used to segment continuous variables into a number of groups or buckets based on the feature value.

34. The CTO of your organization wants to reduce the amount of money spent on running Hadoop clusters in the cloud but does not want to adversely impact the time it takes for jobs to run. When workloads run, they utilize 86% of CPU and 92% of memory. A single cluster is used for all workloads and it runs continuously. What are some options for reducing costs without significantly impacting performance?
The correct answer is to use preemptible worker nodes and use [[ephemeral clusters]].


35. You have been asked to help diagnose a deep learning neural network that has been trained with a large dataset over hundreds of epochs but the accuracy, precision, and recall are below the levels required on both training and test data sets.  You start by reviewing the features and see all the features on numeric. Some are on the scale of 0 to 1, some are on the scale of 0 to 100, and several are on the scale of 0 to 10,000.  What feature engineering technique would you use and why?
The correct answer is normalization, to map all features to the same 0 to 1 scale.


37. A business intelligence analyst is running many [[BigQuery]] queries that are scanning large amounts of data, which leads to higher BigQuery costs. What would you recommend the analyst do to better understand the cost of queries before executing them?
The correct answer is to use the **bq query command with the SQL statement** and the --dry-run option to **return an estimate of the amount of data scanned.**  There is no --estimate option in the bq command.  The --max-rows-per-request sets the maximum number of rows to return per read.  There is no gcloud bigquery command.

38. A business intelligence analyst wants to build a machine learning model to predict the number of units of a product that will be sold in the future based on dozens of features. The features are all stored in a relational database. The business analyst is familiar with reporting tools but not programming in general. What service would you recommend the analyst use to build a model?
The correct answer is[[ AutoML Tables]], which uses structured data to build models with little input from users.  Tensorflow and Spark ML are suitable for modelers with programming skills. There is no Bigtable ML but BigQuery has BigQuery ML.

39. A team of machine learning engineers wants to use [[Kubernetes]] to run their models. They would like to use standard practices for machine learning workflows. What tool would you recommend they use?
The correct answer is [[Kubeflow]], a machine learning toolkit for Kubernetes. Tensorflow and Spark ML are used to build models but are not workflow toolkits. Scikit-Learn is a Python machine learning library.

40. When evaluating a regression model, you notice that small changes in the training data lead to large differences in the predictions. This is an example of what kind of problem?
The correct answer is this is an example of high variance. Low variance is desired and not a problem. High bias occurs when relationships are missed. Low bias is desired and not a problem.

41. A machine learning engineer has built a deep learning network to classify medical radiology images.  When evaluated, the model performed well with 95% accuracy and high precision and recall.  The engineer noted that the training took an unusually long time and asked you how to **decrease the training time without adding additional computing** resources or **risk reducing the quality of the model**. What would you recommend?
The correct answer is to increase the [[learning rate]]. This will allow the model to reach optimal weights faster at the risk of possibly missing the absolute optimal solution. Reducing the number of layers in the model or number of nodes in layers could reduce the quality of the model. Decreasing the learning rate would slow learning.


42. A number of machine learning models used by your company are producing questionable results, particularly with some demographic groups. You suspect there may be an unfairness bias in these models. Which of the following could you use to assess the possibility of unfairness and bias?

The correct answer is **classification parity** which measures the predictive performance across groups. **The more equal the classification parity measure, the less unfair or biased the model is.** Anti-classification is a method of avoiding bias but not assessing bias.  Regularization is a method used to reduce the risk of overfitting. Normalization is a feature engineering technique.


43. A regression model developed three months ago is no longer performing as well as it originally did. What could be the cause of this?

The correct answer is this is an **example of data skew**. The regression model was trained on a set of data that is no longer representative of the data evaluated in production. Underfitting occurs when a model does not capture relationships but this model worked well initially.  Increased latency is not related to the accuracy of models.  Decreased recall is a measure used with classification models, not regression models.

45. Your company has an organization with several folders and several projects defined in the [[Resource Hierarchy]]. You want to limit access to all VMs created within a project. How would you specify those restrictions?
The correct answer is to create a policy and attach it to the project. Attaching a policy to each VM is more work and does not take advantage of inheritance in the resource hierarchy.  There is no need to create custom roles and since the requirement is about access to a kind of resource, a policy is the best approach.

46. An insurance company needs to **keep logs of applications** used to make underwriting decisions. Industry regulations require the company to store logs for seven years. The logs are not likely to be accessed. Approximately 12 TB of log data is generated per year. What is the most cost-effective way to store this data?
The correct answer is Coldline Storage is the least expensive option that meets the requirements. Nearline storage could be used by costs more than Coldline storage. Datastore is a document database and not suitable for storing log data.  Multi-regional storage would meet the storage requirements but would cost more than Coldline storage.

47. A multi-national enterprise used [[Cloud Spanner]] for an inventory management system. After some investigation, you find that hot-spotting is adversely impacting the performance of the Cloud Spanner database.  Which two of the following options could be used to avoid hot-spotting?
[[Hot spotting]] can occur when sequential values are used as primary keys so bit reversing the value and promoting high cardinality attributes in a multi-attribute key will introduce more variation in the sort order of primary keys generated in close proximity.  **Using an auto-incrementing value can actually cause hot-spotting.**  Promoting low cardinality attributes will not introduce as much variation in the sort order as promoting high cardinality values.  Further normalizing the data model will not on its own change the sort order of primary key generation and may not reduce hot spotting.

48. An online gaming company is building a prototype data store for a player information system using [[Cloud Datastore]]. Developers have created a database with 10,000 fictitious player records. The attributes include a player identifier, a list of possessions, a health status, and a team identifier.  Queries that return player identifier and list of possessions filtered by health status return results correctly, however, queries that return player identifier and team identifier filtered by health status and team identifier do not return any results even when there are entities in the database that satisfy the filter. What would you first check when troubleshooting this problem?

The correct answer is to **verify a single composite index** exists on the player identifier and the team identifier. **Cloud Datastore only retrieves results using indexes**, it does not scan entities checking filter conditions. Single attribute indexes are created automatically so any query that references a single attribute will return correct values with no additional indexes.  **Composite conditions that include two or more attributes require composite indexes which must be created manually. Indexed values do not need to be of integer type.  There is no SCAN_ENABLED database parameter in Cloud Datastore.**


50. You have migrated a data warehouse from on-premises to [[BigQuery]]. You have not modified the ETL process other than to change the target database to BigQuery.  The overall load performance is slower than expected and you have been asked to tune the process. You have determined that the most time-consuming part of the load process is the final step of the ETL process. It loads data from CSV files compressed using Snappy compression into BigQuery. The files are stored in[[ Cloud Storage]]. What change would you make to make the load process save the most time in the load process?
The correct answer is to use uncompressed Avro files are the most performant option. Changing the type of compression with CSV files will not increase performance as much as using Avro format.  Both compressed and uncompressed JSON is less performant than Avro format when loading data into BigQuery.







